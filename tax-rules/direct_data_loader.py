#!/usr/bin/env python3
"""
Direct data loader - IRS to BigQuery without Fivetran
"""

import sys
import json
import logging
from datetime import datetime
from typing import Dict, Any, List
from google.cloud import bigquery

# Add src to path
sys.path.append('src')

from tax_rules_connector.connector import TaxRulesConnector, get_default_configuration

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_bigquery_tables():
    """Create BigQuery tables for raw data."""
    client = bigquery.Client(project='province-development')
    
    # Create dataset if it doesn't exist
    dataset_id = 'province-development.raw'
    try:
        client.get_dataset(dataset_id)
        logger.info(f"Dataset {dataset_id} already exists")
    except Exception as e:
        if "Not found" in str(e):
            dataset = bigquery.Dataset(dataset_id)
            dataset.location = "US"
            client.create_dataset(dataset)
            logger.info(f"Created dataset {dataset_id}")
        else:
            logger.info(f"Dataset {dataset_id} already exists")
    
    # Define table schemas
    schemas = {
        'newsroom_releases': [
            bigquery.SchemaField("release_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("title", "STRING"),
            bigquery.SchemaField("url", "STRING"),
            bigquery.SchemaField("published_date", "DATE"),
            bigquery.SchemaField("content_summary", "STRING"),
            bigquery.SchemaField("linked_revproc_url", "STRING"),
            bigquery.SchemaField("keywords_matched", "STRING"),
            bigquery.SchemaField("is_inflation_related", "BOOLEAN"),
            bigquery.SchemaField("is_tax_year_update", "BOOLEAN"),
            bigquery.SchemaField("jurisdiction_level", "STRING"),
            bigquery.SchemaField("jurisdiction_code", "STRING"),
            bigquery.SchemaField("_extracted_at", "TIMESTAMP"),
            bigquery.SchemaField("_source_url", "STRING"),
        ],
        'irb_bulletins': [
            bigquery.SchemaField("bulletin_no", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("doc_number", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("published_date", "DATE"),
            bigquery.SchemaField("doc_type", "STRING"),
            bigquery.SchemaField("title", "STRING"),
            bigquery.SchemaField("url_html", "STRING"),
            bigquery.SchemaField("url_pdf", "STRING"),
            bigquery.SchemaField("jurisdiction_level", "STRING"),
            bigquery.SchemaField("jurisdiction_code", "STRING"),
            bigquery.SchemaField("_extracted_at", "TIMESTAMP"),
            bigquery.SchemaField("_source_url", "STRING"),
        ]
    }
    
    # Create tables
    for table_name, schema in schemas.items():
        table_id = f"{dataset_id}.{table_name}"
        
        try:
            client.get_table(table_id)
            logger.info(f"Table {table_id} already exists")
        except Exception as e:
            if "Not found" in str(e):
                table = bigquery.Table(table_id, schema=schema)
                
                # Add partitioning for performance
                if table_name == 'newsroom_releases':
                    table.time_partitioning = bigquery.TimePartitioning(
                        type_=bigquery.TimePartitioningType.DAY,
                        field="published_date"
                    )
                elif table_name == 'irb_bulletins':
                    table.time_partitioning = bigquery.TimePartitioning(
                        type_=bigquery.TimePartitioningType.DAY,
                        field="published_date"
                    )
                
                client.create_table(table)
                logger.info(f"Created table {table_id}")
            else:
                logger.info(f"Table {table_id} already exists")


def load_data_to_bigquery(data: Dict[str, List[Dict[str, Any]]]):
    """Load extracted data to BigQuery."""
    client = bigquery.Client(project='province-development')
    
    for table_name, records in data.items():
        if not records:
            logger.info(f"No records to load for {table_name}")
            continue
        
        table_id = f"province-development.raw.{table_name}"
        
        try:
            # Convert date objects to strings for BigQuery
            processed_records = []
            for record in records:
                processed_record = {}
                for key, value in record.items():
                    if hasattr(value, 'isoformat'):  # date/datetime object
                        processed_record[key] = value.isoformat()
                    else:
                        processed_record[key] = value
                processed_records.append(processed_record)
            
            # Load data
            job_config = bigquery.LoadJobConfig(
                write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
            )
            
            job = client.load_table_from_json(
                processed_records, 
                table_id, 
                job_config=job_config
            )
            
            job.result()  # Wait for job to complete
            
            logger.info(f"‚úÖ Loaded {len(records)} records to {table_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load data to {table_id}: {e}")


def main():
    """Main pipeline execution."""
    print("üöÄ Direct IRS Tax Rules Pipeline (No Fivetran)")
    print("=" * 60)
    
    # Step 1: Extract data from IRS
    print("üìä Step 1: Extracting data from IRS sources...")
    
    connector = TaxRulesConnector()
    config = get_default_configuration()
    
    # Test connections first
    if not connector.test(config):
        print("‚ùå Connection tests failed!")
        return
    
    # Extract data
    data = connector.extract_data(config, limit_per_stream=10)
    
    total_records = sum(len(records) for records in data.values())
    print(f"‚úÖ Extracted {total_records} total records")
    
    for stream_name, records in data.items():
        print(f"  {stream_name}: {len(records)} records")
    
    if total_records == 0:
        print("‚ö†Ô∏è  No data extracted, creating sample data...")
        # Create sample data for demonstration
        data = {
            'newsroom_releases': [
                {
                    'release_id': 'sample-inflation-2024',
                    'title': 'IRS announces 2024 tax year inflation adjustments',
                    'url': 'https://www.irs.gov/newsroom/sample-inflation-2024',
                    'published_date': datetime(2024, 10, 15).date(),
                    'content_summary': 'The IRS announced inflation adjustments for tax year 2024.',
                    'linked_revproc_url': 'https://www.irs.gov/irb/2024-44_IRB',
                    'keywords_matched': '["inflation", "standard deduction"]',
                    'is_inflation_related': True,
                    'is_tax_year_update': True,
                    'jurisdiction_level': 'federal',
                    'jurisdiction_code': 'US',
                    '_extracted_at': datetime.utcnow().isoformat() + 'Z',
                    '_source_url': 'https://www.irs.gov/newsroom'
                }
            ],
            'irb_bulletins': data.get('irb_bulletins', [])
        }
    
    print()
    
    # Step 2: Create BigQuery tables
    print("üèóÔ∏è  Step 2: Setting up BigQuery tables...")
    create_bigquery_tables()
    print("‚úÖ BigQuery tables ready")
    print()
    
    # Step 3: Load data to BigQuery
    print("üì• Step 3: Loading data to BigQuery...")
    load_data_to_bigquery(data)
    print("‚úÖ Data loaded to BigQuery")
    print()
    
    # Step 4: Verify data
    print("üîç Step 4: Verifying data in BigQuery...")
    client = bigquery.Client(project='province-development')
    
    for table_name in data.keys():
        query = f"SELECT COUNT(*) as count FROM `province-development.raw.{table_name}`"
        result = client.query(query).result()
        count = list(result)[0]['count']
        print(f"  {table_name}: {count} records in BigQuery")
    
    print()
    print("üéâ Direct pipeline completed successfully!")
    print("üìä Data is now ready for dbt transformations")


if __name__ == "__main__":
    main()
